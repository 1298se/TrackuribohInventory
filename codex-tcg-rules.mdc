# Project MDC (Markdown Configuration) Rules

## Project Structure

The project is organized into two main components:

- `web/`: Frontend React/TypeScript application using ShadCN for UI
- `app/`: Backend FastAPI REST API with SQLAlchemy and Pydantic

Supporting directories:

- `core/`: Core functionality and shared services
- `models/`: Database models
- `alembic/`: Database migrations
- `cron/`: Scheduled tasks
- `backfills/`: Backfills

## Coding Standards and Patterns

### Backend (Python)

#### Database Sessions

1. Always use `SessionLocal` from `core.database` to create database sessions
2. Prefer using the context manager pattern with `with SessionLocal() as session:` for automatic cleanup
3. For API endpoints, use dependency injection with `session: Session = Depends(get_db_session)`
4. For scripts and backfills, directly use the context manager pattern
5. Always commit or rollback transactions explicitly before closing the session

Example for scripts:

```python
from core.database import SessionLocal

def my_function():
    with SessionLocal() as session:
        # Database operations
        items = session.query(MyModel).all()

        # Make changes
        for item in items:
            item.some_field = new_value

        # Commit changes
        session.commit()
```

Example for API endpoints:

```python
@router.post("/items/")
def create_item(
    item: ItemCreateSchema,
    session: Session = Depends(get_db_session)
):
    db_item = MyModel(**item.dict())
    session.add(db_item)
    session.commit()
    return db_item
```

#### API Endpoints

1. All API endpoints are defined in `app/routes/{module}/api.py` files
2. Each module has its own router with a specific prefix
3. Endpoint functions should include appropriate response_model annotations
4. Use dependency injection with Depends() for services and database sessions
5. Include type annotations for all parameters

Example:

```python
@router.get("/", response_model=InventoryResponseSchema)
def get_inventory(
    background_tasks: BackgroundTasks,
    catalog_service: TCGPlayerCatalogService = Depends(get_tcgplayer_catalog_service),
    session: Session = Depends(get_db_session),
):
    # Implementation
    return InventoryResponseSchema(
        inventory_items=inventory_items
    )
```

#### Schemas

1. All schemas are defined in `app/routes/{module}/schemas.py` files
2. Use Pydantic BaseModel for all request/response schemas
3. Schema class names should end with `Schema` suffix
4. Response schemas should end with `ResponseSchema`
5. Request schemas should end with `RequestSchema`
6. Include type annotations for all fields

Example:

```python
class InventoryItemResponseSchema(BaseModel):
    sku: SKUWithProductResponseSchema
    quantity: int
    average_cost_per_item: MoneySchema
    lowest_listing_price: MoneySchema | None

class InventoryResponseSchema(BaseModel):
    inventory_items: list[InventoryItemResponseSchema]
```

### Frontend (TypeScript)

#### General Preferences

1. **Prefer `function` declarations over `const` arrow functions** when possible
   - Use `function` for component definitions: `export function MyComponent() { ... }`
   - Use `function` for standalone functions: `function formatCurrency(amount: number) { ... }`
   - Arrow functions are acceptable for: callbacks, inline functions, and when you need to preserve `this` context

Example:

```typescript
// Preferred
export function ProductCard({ product }: Props) {
  return <div>{product.name}</div>;
}

function calculateTotal(items: Item[]) {
  return items.reduce((sum, item) => sum + item.price, 0);
}

// Acceptable for callbacks
items.map((item) => item.price);
```

#### UI Components

1. All ShadCN UI components are stored in the `/web/components/ui` directory
2. Custom components based on ShadCN should follow the same styling patterns
3. Reuse UI components whenever possible to maintain consistency

#### API Hooks

1. All API hooks are defined in `web/app/{module}/api.ts` files
2. Use SWR for data fetching and caching
3. Each hook function should have a clear name that matches its purpose
4. Include proper type annotations for parameters and return values
5. Use proper TypeScript parameter typing

Example:

```typescript
export function useSearchProducts(
  query: string,
  catalog: string | null = null,
  productType: string | null = null
) {
  const params: { [key: string]: string } = { query };

  if (catalog) {
    params.catalog_id = catalog;
  }

  if (productType) {
    params.product_type = productType;
  }

  return useSWR<ProductSearchResponse>(
    {
      url: `${API_URL}/catalog/search`,
      params,
    },
    fetcher
  );
}
```

#### Schemas

1. All schemas are defined in `web/app/{module}/schemas.ts` files
2. Use Zod for schema validation and type generation
3. Group related schemas together with comments
4. Export type inferences at the end of the file
5. Schema names should match their backend counterparts

Example:

```typescript
export const InventoryItemResponseSchema = z.object({
  sku: SKUWithProductResponseSchema,
  quantity: z.number(),
  average_cost_per_item: MoneySchema,
  lowest_listing_price: MoneySchema.nullable(),
});

export const InventoryResponseSchema = z.object({
  inventory_items: z.array(InventoryItemResponseSchema),
});

export type InventoryItemResponse = z.infer<typeof InventoryItemResponseSchema>;
export type InventoryResponse = z.infer<typeof InventoryResponseSchema>;
```

## Schema Synchronization

1. TypeScript schemas must match their Python counterparts exactly
2. Field names must be identical
3. Field types must be equivalent (e.g., Python `int` = TypeScript `z.number()`)
4. Optional fields in Python (`field: Type | None`) should be nullable in TypeScript (`field: Schema.nullable()`)
5. When adding a new field, add it to both schemas simultaneously

## Database Migrations

### Alembic Migration Workflow

1. **Never create migration files manually** - Always use Alembic's autogenerate feature
2. **Update models first** - Make changes to SQLAlchemy models in `core/models/`
3. **Generate migration** - Run `alembic revision --autogenerate -m "description of changes"`
4. **Review the generated migration** - Check the auto-generated file for accuracy
5. **Do not modify migration files** - Only modify in extremely special circumstances, which must be communicated clearly

### Migration Commands

```bash
# Generate a new migration after model changes
alembic revision --autogenerate -m "add user email field"

# Apply migrations
alembic upgrade head

# Rollback one migration
alembic downgrade -1

# View migration history
alembic history
```

### Important Migration Rules

1. Always review auto-generated migrations before applying them
2. Test migrations on a development database first
3. Never modify migration files after they've been applied to production
4. If a migration needs modification, create a new migration instead
5. Keep migration descriptions clear and concise

## Testing

1. Backend API tests should be placed in `app/routes/{module}/test_api.py`
2. Test coverage should be maintained for all critical paths

## Naming Conventions

1. Use snake_case for Python variables, functions, and file names
2. Use camelCase for TypeScript variables and functions
3. Use PascalCase for classes and types in both languages
4. Backend endpoint paths should use kebab-case

5. Use kebab-case for frontend file names (TypeScript/TSX, CSS, hooks, components), e.g., `product-card.tsx`, `use-debounce.ts`
6. Follow Alembic's default migration file naming: `<revisionID>_<description>.py`, e.g., `6702d39d0232_create_inventory_snapshot_table.py`

## Documentation

1. Include docstrings for all Python functions
2. Document complex TypeScript functions with JSDoc comments
3. Maintain accurate README files for each major component

## Version Control

1. Commit messages should be clear and descriptive
2. Do not use prefixes like `feat:`, `fix:`, etc. (Conventional Commits style).
3. Include the affected module in commit messages when possible
4. Group related changes in a single commit

## Dependencies

1. Python dependencies are defined in `pyproject.toml` and managed with uv.
2. Frontend dependencies are managed with npm (package.json)
3. Specify exact versions for critical dependencies

## Operational Commands

### Manually Run Update Catalog Task

Use this command to manually trigger the `update_catalog_db` ECS task (replace task definition revision if needed):

```bash
aws ecs run-task \
  --cluster codex-tcg-cron-cluster \
  --task-definition arn:aws:ecs:us-east-2:235494812649:task-definition/codex-tcg-update-catalog:2 \
  --launch-type FARGATE \
  --network-configuration "awsvpcConfiguration={subnets=[subnet-095f4b7e5c8eef77d,subnet-002b23ebe0247ec0c,subnet-04d6c95985fe61856],securityGroups=[sg-00ec8440724857d7d],assignPublicIp=ENABLED}" \
  --region us-east-2
```

## Debugging Commands

### Manually Run ECS Update Catalog Task

To manually trigger the catalog update task (e.g., for testing), use the following command, ensuring the cluster name, task definition ARN (including the correct revision), subnets, and security group ID are accurate:

```bash
aws ecs run-task \
  --cluster codex-tcg-cron-cluster \
  --task-definition arn:aws:ecs:us-east-2:235494812649:task-definition/codex-tcg-update-catalog:2 \
  --launch-type FARGATE \
  --network-configuration "awsvpcConfiguration={subnets=[subnet-095f4b7e5c8eef77d,subnet-002b23ebe0247ec0c,subnet-04d6c95985fe61856],securityGroups=[sg-00ec8440724857d7d],assignPublicIp=ENABLED}" \
  --region us-east-2
```

### Prompts

#### Implementing

Let's start with the implementation. Please follow the outlined steps in {spec} very closely. Please do not make any assumptions, and ask clarifying questions when needed

#### Spec + Implementation

We are going to work on showing product price history to users. Please keep asking me questions to clearly define the feature and the implementation.

Please write up a spec in the .tmp directory.

Some initial thoughts I have are:

1. SKU pricing data will come from @tcgplayer_catalog_service.py . This function handles bulk SKU queries, although it uses a GET fetch with parameters, so the number of SKUs passed in a query is limited.
2. Ideally, we would want to store a snapshot of SKU pricing data for every SKU in our database. However, there are way too many (643300 total SKUs).
3. Most SKUs are not important (i.e. Moderately played, heavily played, damaged conditions).
